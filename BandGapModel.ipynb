{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, csv_file):\n",
    "    self.file = pd.read_csv(csv_file)\n",
    "    self.file.dropna()\n",
    "    self.file[\"Data\"] = self.file[\"Data\"].apply(ast.literal_eval)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.file)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "\n",
    "    row = self.file.iloc[idx]\n",
    "    features = torch.tensor(row[\"Data\"], dtype=torch.float32)\n",
    "    target = torch.tensor(row[\"BandGap\"], dtype=torch.float32)\n",
    "\n",
    "    return features, target\n",
    "\n",
    "csv_file = \"UpdatedMaterialsDF.csv\"\n",
    "\n",
    "BG_DataSet = CustomDataset(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingSplit = int(0.8*len(BG_DataSet))\n",
    "TestingSplit = len(BG_DataSet) - TrainingSplit\n",
    "\n",
    "Batch_Size = 32\n",
    "\n",
    "Train_Dataset, Testing_Dataset = torch.utils.data.random_split(BG_DataSet, [TrainingSplit, TestingSplit])\n",
    "\n",
    "Train_Loader = DataLoader(Train_Dataset, batch_size=Batch_Size, shuffle=True)\n",
    "\n",
    "Testing_Loader = DataLoader(Testing_Dataset, batch_size=Batch_Size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, target in Train_Loader:\n",
    "    assert not torch.isnan(features).any(), \"Training data contains NaNs\"\n",
    "    assert not torch.isnan(target).any(), \"Training labels contain NaNs\"\n",
    "for features, target in Testing_Loader:\n",
    "    assert not torch.isnan(features).any(), \"Testing data contains NaNs\"\n",
    "    assert not torch.isnan(target).any(), \"Testing labels contain NaNs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFFNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleFFNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_size = 7  # Number of features\n",
    "hidden_size = 32\n",
    "output_size = 1\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFFNN(input_size, hidden_size, output_size)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.7159, Testing Loss: 1.6321\n",
      "Epoch 2/100, Training Loss: 1.5679, Testing Loss: 1.5595\n",
      "Epoch 3/100, Training Loss: 1.5148, Testing Loss: 1.5236\n",
      "Epoch 4/100, Training Loss: 1.4908, Testing Loss: 1.4979\n",
      "Epoch 5/100, Training Loss: 1.4703, Testing Loss: 1.4799\n",
      "Epoch 6/100, Training Loss: 1.4536, Testing Loss: 1.4621\n",
      "Epoch 7/100, Training Loss: 1.4385, Testing Loss: 1.4480\n",
      "Epoch 8/100, Training Loss: 1.4300, Testing Loss: 1.4618\n",
      "Epoch 9/100, Training Loss: 1.4223, Testing Loss: 1.4433\n",
      "Epoch 10/100, Training Loss: 1.4161, Testing Loss: 1.4601\n",
      "Epoch 11/100, Training Loss: 1.4123, Testing Loss: 1.4234\n",
      "Epoch 12/100, Training Loss: 1.4090, Testing Loss: 1.4194\n",
      "Epoch 13/100, Training Loss: 1.4048, Testing Loss: 1.4183\n",
      "Epoch 14/100, Training Loss: 1.4012, Testing Loss: 1.4101\n",
      "Epoch 15/100, Training Loss: 1.3978, Testing Loss: 1.4099\n",
      "Epoch 16/100, Training Loss: 1.3958, Testing Loss: 1.4080\n",
      "Epoch 17/100, Training Loss: 1.3936, Testing Loss: 1.4016\n",
      "Epoch 18/100, Training Loss: 1.3911, Testing Loss: 1.3983\n",
      "Epoch 19/100, Training Loss: 1.3878, Testing Loss: 1.3978\n",
      "Epoch 20/100, Training Loss: 1.3833, Testing Loss: 1.3918\n",
      "Epoch 21/100, Training Loss: 1.3810, Testing Loss: 1.3953\n",
      "Epoch 22/100, Training Loss: 1.3769, Testing Loss: 1.4097\n",
      "Epoch 23/100, Training Loss: 1.3751, Testing Loss: 1.3810\n",
      "Epoch 24/100, Training Loss: 1.3726, Testing Loss: 1.3842\n",
      "Epoch 25/100, Training Loss: 1.3688, Testing Loss: 1.3864\n",
      "Epoch 26/100, Training Loss: 1.3657, Testing Loss: 1.3899\n",
      "Epoch 27/100, Training Loss: 1.3633, Testing Loss: 1.3723\n",
      "Epoch 28/100, Training Loss: 1.3614, Testing Loss: 1.3738\n",
      "Epoch 29/100, Training Loss: 1.3575, Testing Loss: 1.3717\n",
      "Epoch 30/100, Training Loss: 1.3560, Testing Loss: 1.3640\n",
      "Epoch 31/100, Training Loss: 1.3546, Testing Loss: 1.3648\n",
      "Epoch 32/100, Training Loss: 1.3510, Testing Loss: 1.3574\n",
      "Epoch 33/100, Training Loss: 1.3493, Testing Loss: 1.3510\n",
      "Epoch 34/100, Training Loss: 1.3469, Testing Loss: 1.3533\n",
      "Epoch 35/100, Training Loss: 1.3437, Testing Loss: 1.3557\n",
      "Epoch 36/100, Training Loss: 1.3427, Testing Loss: 1.3672\n",
      "Epoch 37/100, Training Loss: 1.3418, Testing Loss: 1.3501\n",
      "Epoch 38/100, Training Loss: 1.3378, Testing Loss: 1.3573\n",
      "Epoch 39/100, Training Loss: 1.3363, Testing Loss: 1.3456\n",
      "Epoch 40/100, Training Loss: 1.3342, Testing Loss: 1.3401\n",
      "Epoch 41/100, Training Loss: 1.3342, Testing Loss: 1.3714\n",
      "Epoch 42/100, Training Loss: 1.3324, Testing Loss: 1.3441\n",
      "Epoch 43/100, Training Loss: 1.3315, Testing Loss: 1.3407\n",
      "Epoch 44/100, Training Loss: 1.3300, Testing Loss: 1.3304\n",
      "Epoch 45/100, Training Loss: 1.3295, Testing Loss: 1.3333\n",
      "Epoch 46/100, Training Loss: 1.3283, Testing Loss: 1.3373\n",
      "Epoch 47/100, Training Loss: 1.3268, Testing Loss: 1.3322\n",
      "Epoch 48/100, Training Loss: 1.3249, Testing Loss: 1.3290\n",
      "Epoch 49/100, Training Loss: 1.3244, Testing Loss: 1.3340\n",
      "Epoch 50/100, Training Loss: 1.3236, Testing Loss: 1.3270\n",
      "Epoch 51/100, Training Loss: 1.3219, Testing Loss: 1.3258\n",
      "Epoch 52/100, Training Loss: 1.3215, Testing Loss: 1.3256\n",
      "Epoch 53/100, Training Loss: 1.3201, Testing Loss: 1.3312\n",
      "Epoch 54/100, Training Loss: 1.3202, Testing Loss: 1.3251\n",
      "Epoch 55/100, Training Loss: 1.3189, Testing Loss: 1.3297\n",
      "Epoch 56/100, Training Loss: 1.3183, Testing Loss: 1.3243\n",
      "Epoch 57/100, Training Loss: 1.3174, Testing Loss: 1.3265\n",
      "Epoch 58/100, Training Loss: 1.3165, Testing Loss: 1.3278\n",
      "Epoch 59/100, Training Loss: 1.3164, Testing Loss: 1.3237\n",
      "Epoch 60/100, Training Loss: 1.3154, Testing Loss: 1.3174\n",
      "Epoch 61/100, Training Loss: 1.3137, Testing Loss: 1.3176\n",
      "Epoch 62/100, Training Loss: 1.3133, Testing Loss: 1.3174\n",
      "Epoch 63/100, Training Loss: 1.3131, Testing Loss: 1.3266\n",
      "Epoch 64/100, Training Loss: 1.3130, Testing Loss: 1.3180\n",
      "Epoch 65/100, Training Loss: 1.3119, Testing Loss: 1.3155\n",
      "Epoch 66/100, Training Loss: 1.3124, Testing Loss: 1.3242\n",
      "Epoch 67/100, Training Loss: 1.3103, Testing Loss: 1.3406\n",
      "Epoch 68/100, Training Loss: 1.3104, Testing Loss: 1.3389\n",
      "Epoch 69/100, Training Loss: 1.3104, Testing Loss: 1.3177\n",
      "Epoch 70/100, Training Loss: 1.3096, Testing Loss: 1.3122\n",
      "Epoch 71/100, Training Loss: 1.3103, Testing Loss: 1.3099\n",
      "Epoch 72/100, Training Loss: 1.3100, Testing Loss: 1.3190\n",
      "Epoch 73/100, Training Loss: 1.3091, Testing Loss: 1.3120\n",
      "Epoch 74/100, Training Loss: 1.3083, Testing Loss: 1.3114\n",
      "Epoch 75/100, Training Loss: 1.3072, Testing Loss: 1.3223\n",
      "Epoch 76/100, Training Loss: 1.3073, Testing Loss: 1.3101\n",
      "Epoch 77/100, Training Loss: 1.3059, Testing Loss: 1.3123\n",
      "Epoch 78/100, Training Loss: 1.3057, Testing Loss: 1.3105\n",
      "Epoch 79/100, Training Loss: 1.3050, Testing Loss: 1.3152\n",
      "Epoch 80/100, Training Loss: 1.3051, Testing Loss: 1.3089\n",
      "Epoch 81/100, Training Loss: 1.3052, Testing Loss: 1.3181\n",
      "Epoch 82/100, Training Loss: 1.3060, Testing Loss: 1.3185\n",
      "Epoch 83/100, Training Loss: 1.3051, Testing Loss: 1.3058\n",
      "Epoch 84/100, Training Loss: 1.3033, Testing Loss: 1.3072\n",
      "Epoch 85/100, Training Loss: 1.3046, Testing Loss: 1.3076\n",
      "Epoch 86/100, Training Loss: 1.3036, Testing Loss: 1.3051\n",
      "Epoch 87/100, Training Loss: 1.3021, Testing Loss: 1.3166\n",
      "Epoch 88/100, Training Loss: 1.3035, Testing Loss: 1.3239\n",
      "Epoch 89/100, Training Loss: 1.3022, Testing Loss: 1.3199\n",
      "Epoch 90/100, Training Loss: 1.3010, Testing Loss: 1.3241\n",
      "Epoch 91/100, Training Loss: 1.3013, Testing Loss: 1.3062\n",
      "Epoch 92/100, Training Loss: 1.3005, Testing Loss: 1.3173\n",
      "Epoch 93/100, Training Loss: 1.2997, Testing Loss: 1.3048\n",
      "Epoch 94/100, Training Loss: 1.2995, Testing Loss: 1.3045\n",
      "Epoch 95/100, Training Loss: 1.2995, Testing Loss: 1.3217\n",
      "Epoch 96/100, Training Loss: 1.3000, Testing Loss: 1.2997\n",
      "Epoch 97/100, Training Loss: 1.2986, Testing Loss: 1.3003\n",
      "Epoch 98/100, Training Loss: 1.2980, Testing Loss: 1.3152\n",
      "Epoch 99/100, Training Loss: 1.2990, Testing Loss: 1.3043\n",
      "Epoch 100/100, Training Loss: 1.2980, Testing Loss: 1.3099\n"
     ]
    }
   ],
   "source": [
    "# Lists to store losses\n",
    "list_loss_training = []\n",
    "list_epoch = []\n",
    "list_loss_testing = []\n",
    "\n",
    "def train(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for features, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = loss_fn(output.squeeze(), target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def test(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_loader:\n",
    "            output = model(features)\n",
    "            loss = loss_fn(output.squeeze(), target)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(test_loader)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, Train_Loader, loss_fn, optimizer)\n",
    "    test_loss = test(model, Testing_Loader, loss_fn)\n",
    "\n",
    "    list_loss_training.append(train_loss)\n",
    "    list_loss_testing.append(test_loss)\n",
    "    list_epoch.append(epoch + 1)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss:.4f}, Testing Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossOutFile = \"LossCSV.csv\"\n",
    "\n",
    "LossDF = pd.DataFrame({\"Epoch\" : list_epoch, \"Training Loss\" : list_loss_training, \"Testing_Loss\" : list_loss_testing})\n",
    "LossDF.to_csv(LossOutFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f=\"SimpleFFNN_BG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
